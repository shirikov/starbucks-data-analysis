{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# load data\n",
    "offer_data = pd.read_csv('data/cleaned_offer_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook documents an analysis of the probability of completing various promotional offers based on Starbucks data. The notebook uses a cleaned data set on offers received by each user that was created in a separate notebook (starbucks_data_cleanup.ipynb).\n",
    "\n",
    "The overall idea for the analysis is to use a machine learning algorithm to predict whether users complete (redeem) the offers from existing data on users and offers. We can then use models with sufficient predictive power to compare predicted probabilities of completing the offers for different segments of customers or for offers with different characteristics. In this analysis, we will mainly use logistic regression, as it usually performs well with respect to binary predictions, and it also allows us to examine the contribution of each predictor by looking at regression coefficients. We will also look at who is more likely to view offers using the same model setup.\n",
    "\n",
    "At the end of the analysis, there is a discussion of the takeaways and possible improvements.\n",
    "\n",
    "# Setting Up\n",
    "\n",
    "First, we create several dummy variables for segments of the customer base based on age, income, gender, and when one became a member. As for age, standard age group ranges (0--20, 21--30, etc.) were used. Incomes in the data set were in the range from 30,000 to 120,000, so 20K brackets were used except for the first one (0--40). Membership duration was split into five bins: 0-3 months, 4-6 months, 7-12 months, 1-2 years, more than 2 years. We also add dummies for offer types: BOGO, discount, and informational (informational offers will not be used in this analysis as they cannot be 'completed')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn continuous variables to categorical\n",
    "offer_data['age_group'] = pd.cut(\n",
    "    offer_data.age, bins=[0, 20, 30, 40, 50, 60, 150],\n",
    "    labels=['18-20', '21-30', '31-40', '41-50', '51-60', '61+'])\n",
    "offer_data['income_group'] = pd.cut(\n",
    "    offer_data.income, bins=[0, 40000, 60000, 80000, 100000, 120000],\n",
    "    labels=['0-40K', '41-60K', '61-80K', '81-100K', '101-120K'])\n",
    "offer_data['member_months_group'] = pd.cut(\n",
    "    offer_data.member_months, bins=[0, 3, 6, 12, 24, 1000],\n",
    "    labels=['0-3 months', '4-6 months', '7-12 months', \n",
    "            '1-2 years', '2+ years'])\n",
    "\n",
    "# recode offer ids\n",
    "offer_data['offer_id_alt'] = (offer_data.offer_type + ' ') + (\n",
    "    offer_data.offer_reward.astype(str) + ' (') + (\n",
    "        (offer_data.offer_duration/24).astype(int).astype(str) + ' days)')\n",
    "        \n",
    "# drop informational offers\n",
    "offer_data = offer_data[offer_data.offer_type != 'informational']\n",
    "\n",
    "# create dummies\n",
    "offer_data = pd.concat([offer_data,\n",
    "                            pd.get_dummies(offer_data['age_group'],\n",
    "                                           prefix='Age'),\n",
    "                            pd.get_dummies(offer_data['income_group'],\n",
    "                                           prefix='Income'),\n",
    "                            pd.get_dummies(offer_data['gender'],\n",
    "                                           prefix='Gender'),\n",
    "                            pd.get_dummies(offer_data['member_months_group'], \n",
    "                                           prefix='Member'),\n",
    "                            pd.get_dummies(offer_data['offer_type'], \n",
    "                                           prefix='Type'),\n",
    "                            pd.get_dummies(offer_data['offer_id_alt'], \n",
    "                                           prefix='Offer')],\n",
    "                            axis=1)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up functions to split the data into train and test sets and to fit the models.\n",
    "\n",
    "In this case, instead of using the standard train_test_split function, it is reasonable to split the data into train and test sets at the level of the user. For this purpose, we use a custom splitting function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_by_user(X, y):\n",
    "    \n",
    "    '''\n",
    "    Create a train and test list of user ids, then split the data accordingly.\n",
    "    \n",
    "    Args:\n",
    "        X: a data frame of predictors\n",
    "        y: the target variable (pandas series)\n",
    "    '''\n",
    "    \n",
    "    # create train and test sets of users\n",
    "    train_ix, test_ix = next(gss.split(X, y, groups=X.user_id))\n",
    "    \n",
    "    # split into train and test\n",
    "    X = X.drop(columns='user_id')\n",
    "    X_train = X.iloc[train_ix]\n",
    "    y_train = y.iloc[train_ix]\n",
    "    X_test = X.iloc[test_ix]\n",
    "    y_test = y.iloc[test_ix]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# initialize the command for splitting the data\n",
    "gss = GroupShuffleSplit(n_splits=2, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function fits the models, outputs model evaluation, and, in case of logistic regression models, saves the regression coefficients for the subsequent interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_test(model_pipeline, return_coefs=False):\n",
    "    \n",
    "    '''\n",
    "    Fit the model, print out model evaluations on test data,\n",
    "    return coefficients.\n",
    "    \n",
    "    Args:\n",
    "        model_pipeline: a pipeline/a model object\n",
    "    '''\n",
    "    \n",
    "    # Fit the model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Test the model\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    \n",
    "    print('F1 score:')\n",
    "    print(round(f1_score(y_test, y_pred), 3))\n",
    "    print('Precision:')\n",
    "    print(round(precision_score(y_test, y_pred), 3))\n",
    "    print('Recall:')\n",
    "    print(round(recall_score(y_test, y_pred), 3))\n",
    "    \n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    ConfusionMatrixDisplay.from_estimator(model_pipeline, X_test, y_test, \n",
    "                                          normalize='all')\n",
    "    \n",
    "    if return_coefs:\n",
    "        coef_list = [recode_coefs(c) for c in X_train.columns.tolist()]\n",
    "        coefs = pd.DataFrame({'Coefficient': model_pipeline.named_steps[\n",
    "                              'logit'].coef_.flatten()},\n",
    "                              index=coef_list)\n",
    "        return coefs\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# recode variable names\n",
    "def recode_coefs(coef):\n",
    "    \n",
    "    '''\n",
    "    Relabels variable names for plots.\n",
    "    '''\n",
    "    coef = coef.replace('Offer_', '')\n",
    "    coef = coef.replace('_', ' ')\n",
    "    coef = coef.replace('bogo', 'BOGO')\n",
    "    coef = coef.replace('discount', 'Discount')\n",
    "    coef = coef.replace('any', 'Any')\n",
    "    coef = coef.replace('same', 'Same')\n",
    "    coef = coef.replace('saw', 'Saw')\n",
    "    \n",
    "    return(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we set up two model pipelines, one using logistic regression, and another using gradient boosting. In both cases, data are normalized using a MinMaxScaler from sklearn, which rescales each variable to take values from 0 to 1. This is more appropriate in this case compared to standard centering and scaling because many of the variables are dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model pipelines\n",
    "pipeline_logit = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('logit', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline_boost = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('boost', GradientBoostingClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis below, we will use the following set of predictors:\n",
    "- Dummies for promo campaigns (offers). While the data contain several characteristics for each offer, such as its type, reward, duration, and required spending, there are just 8 offers in this analysis (after dropping 2 informational ones), and including all these specific characteristics will be essentially the same as including just the offer ids, as the set of these features allows to identify each offer precisely (see the list of offers below). It would thus be easier to interpret the results if we include just the dummies for each offer. The dummy 'bogo 5 (5 days)' is omitted and serves as the reference category.\n",
    "- The count of previously completed offers by the same user (one variable counts offers with the same id, the other variable counts all previously completed offers). This allows us to capture whether one tends to spend more or to seek offers more actively.\n",
    "- Age groups. The group 'Age 18-20' serves as a reference category.\n",
    "- Income groups. The group '0-40K' serves as a reference category.\n",
    "- Gender. 'Male' is the reference category.\n",
    "- How long one has been a member. '0-3 months' is a reference category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offers in the analysis (the number after offer type is reward value in dollars):\n",
      "['bogo 10 (5 days)', 'bogo 10 (7 days)', 'bogo 5 (5 days)', 'bogo 5 (7 days)', 'discount 2 (10 days)', 'discount 2 (7 days)', 'discount 3 (7 days)', 'discount 5 (10 days)']\n"
     ]
    }
   ],
   "source": [
    "print('Offers in the analysis (the number after offer type is reward value in dollars):')\n",
    "print(offer_data.sort_values('offer_id_alt').offer_id_alt.unique().tolist())\n",
    "\n",
    "x_vars = ['Offer_bogo 10 (5 days)', 'Offer_bogo 10 (7 days)', \n",
    "          'Offer_bogo 5 (7 days)',\n",
    "          'Offer_discount 2 (7 days)',  'Offer_discount 2 (10 days)',\n",
    "          'Offer_discount 3 (7 days)', 'Offer_discount 5 (10 days)', \n",
    "          'same_offer_completed_before', 'any_offer_completed_before',\n",
    "          'Age_21-30', 'Age_31-40', 'Age_41-50', 'Age_51-60', 'Age_61+',\n",
    "          'Income_41-60K', 'Income_61-80K', 'Income_81-100K', \n",
    "          'Income_101-120K', \n",
    "          'Gender_Female', 'Gender_Other', \n",
    "          'Member_4-6 months', 'Member_7-12 months',\n",
    "          'Member_1-2 years', 'Member_2+ years']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a matrix of predictors X and the outcome variable, and then we split the data into the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y\n",
    "offer_data_X = offer_data[['user_id'] + x_vars]\n",
    "offer_data_y = offer_data['completed']\n",
    "\n",
    "# train and test\n",
    "X_train, y_train, X_test, y_test = train_test_split_by_user(offer_data_X,\n",
    "                                                            offer_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Finding the Best-Performing Model\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "The logistic regression implementation in sklearn by default includes a regularization penalty, but the strength of regularization could be tuned via the parameter called C. First, we use grid search with cross-validation to find the optimal value of C.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:\n",
      "0.799\n",
      "Precision:\n",
      "0.774\n",
      "Recall:\n",
      "0.826\n",
      "Confusion matrix:\n",
      "[[3557 2401]\n",
      " [1733 8221]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid search for optimal model parameters\n",
    "parameters_logit = {\n",
    "        'logit__C': [0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000]\n",
    "    }\n",
    "logit_cv = GridSearchCV(pipeline_logit, param_grid=parameters_logit)\n",
    "fit_test(logit_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model has an F1 score of 0.8. F1 score is a single metric that takes into account both model precision and recall, where precision shows what percent of the offers that the model predicted to be completed were actually completed, and recall shows what percent of all completed offers the model was able to predict as completed. It is important to consider both precision and recall because we want our models to minimize both false positives and false negatives, and F1 score is a convenient combination of these metrics. However, in addition, this code also prints out precision and recall. \n",
    "\n",
    "Looking at precision and recall is also important in our specific case because there is some class imbalance in the data: offers were completed in about 63 percent of cases (see the confusion matrix for the total number of true positives). This might bias the model in favor of positive results. We can see that there is some such bias indeed, both from the confusion matrix where there are more false positives than false negatives, and from the fact that model precision is somewhat lower than recall. But this bias is not too severe, given the relatively small difference between two metrics.\n",
    "\n",
    "As for the optimal value of C, it is 10, as shown below, so this is what we will use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameter: {'logit__C': 10}\n"
     ]
    }
   ],
   "source": [
    "print('Optimal parameter: ' + str(logit_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "As an alternative, we'll consider another classifier, gradient boosting, also implemented in sklearn. This is an ensemble tree-based model. In this case, we'll tune three parameters: learning rate (the contribution of each tree to final predictions), the number of estimators (boosting stages), and max depth (the number of nodes in each tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:\n",
      "0.803\n",
      "Precision:\n",
      "0.783\n",
      "Recall:\n",
      "0.824\n",
      "Confusion matrix:\n",
      "[[3682 2276]\n",
      " [1750 8204]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid search for optimal parameters\n",
    "parameters_boost = {\n",
    "        'boost__max_depth': [1, 2, 3, 4, 5],\n",
    "        'boost__n_estimators': [10, 50, 100, 200],\n",
    "        'boost__learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    }\n",
    "boost_cv = GridSearchCV(pipeline_boost, param_grid=parameters_boost)\n",
    "fit_test(boost_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the optimal parameters for this model. As the results above show, the F1 score of the gradient boosting classifier is almost the same as in the logistic regression case above (slightly lower recall and slightly higher precision). Given that the logistic regression model allows for easier interpretation of results based on regression coefficients, we will use it for the remaining analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boost__learning_rate': 0.2, 'boost__max_depth': 3, 'boost__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(boost_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Offer Completion\n",
    "\n",
    "Now, let's examine the regression coefficients using the tuned logistic model. It is not straightforward to derive the exact numeric differences in the probability of completing the offer between customer segments or offers because the predictor variables were rescaled. Rather, the goal of this discussion is to provide some intuitive understanding about when this probability is higher. A positive coefficient means that the probability of completing the offer is higher for a particular category of users or offers compared to the reference category. The larger the coefficient, the more likely is the offer to be completed. And vice versa, negative coefficients mean lower probability of completing offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting completion\n",
    "completion_coefs = fit_test(logit_cv.best_estimator_, return_coefs=True)\n",
    "\n",
    "completion_coefs.plot.barh(figsize=[15,5], \n",
    "                           ylabel='Logistic regression coefficient', \n",
    "                           title='What predicts offer completion?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth keeping in mind that when we have several dummies, e.g., for the duration of membership, the coefficients on these dummies reflect how the corresponding categories related to the omitted category. In case of membership, the reference category is those who have been members for 0--3 months. Compared to them, all other groups of members were *more* likely to complete offers. But members with a longer track record were in general more likely to redeem offers, even compared to those who were members for 4--6 months or 7--12 months. \n",
    "\n",
    "Similarly, compared to males (omitted category), female customers and those who specified their gender as 'other' were more likely to complete offers, as were customers who earned more compared to those with lower income. As for age, customers over 40 were more likely to complete offers than customers between 18 and 30 years old.\n",
    "\n",
    "Further, customers who have previously completed either the same offer or any other offer were also more likely to redeem offers. As noted before, either these customers generally make orders more frequently, or these are individuals who tend to look out for promo offers and make use of them.\n",
    "\n",
    "As for specific offers, there was no clear pattern. While two discount offers turned out to be more effective than the rest, two other discount offers were less popular---about the same as a couple of BOGO offers. It does not seem like there is a pattern with respect to reward size or offer duration. The only possible conclusion is that discount offers may be somewhat more attractive than BOGO offers, which makes sense.\n",
    "\n",
    "## Accounting for the Knowledge of the Offer\n",
    "\n",
    "A flaw of the model described above is that it considered all customers at once without distinguishing between those who did and did not see the offers in question. But some people would complete offers even without knowing about them just because they spend some amount regularly. What we want to know is whether an offer makes it more likely that people would spend more. So one possibility is to consider customers who did not see the offer to be spending as they would usually spend, whereas customers who saw the offer would be spending under the influence of this offer. (Ideally, of course, we would use a different study setup, on which see below.)\n",
    "\n",
    "There are different ways in which we can tackle this issue. First, we can simply add a dummy that indicates whether one saw the offer or not to the regression and check the coefficient. To do this, we add a new variable *saw_offer* that takes the value of 1 if the customer viewed the offer and then completed it or viewed the offer and did not complete it; the variable takes the value of 0 if the customer viewed the offer after completing it or did not view it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_data['saw_offer'] = (\n",
    "    (offer_data.viewed == 1) &\n",
    "    (offer_data.viewed_before != 0)).astype(int)\n",
    "\n",
    "offer_data_X = offer_data[['user_id'] + x_vars + ['saw_offer']]\n",
    "offer_data_y = offer_data['completed']\n",
    "\n",
    "# train and test\n",
    "X_train, y_train, X_test, y_test = train_test_split_by_user(offer_data_X,\n",
    "                                                            offer_data_y)\n",
    "\n",
    "# fit \n",
    "completion_coefs_saw = fit_test(logit_cv.best_estimator_, return_coefs=True)\n",
    "\n",
    "completion_coefs_saw.plot.barh(figsize=[15,5], \n",
    "                               ylabel='Logistic regression coefficient', \n",
    "                               title='What predicts offer completion?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient on whether one saw the offer is somewhat positive, but its magnitude is small compared to the differences between various other customer segments. Thus, we might think that those who actually saw promo offers were not *much* more likely to complete them.\n",
    "\n",
    "Another way to look at this is to complete the average probability of completing the offer for those who saw them and those who did not see them. Again, not a dramatic difference, although it is probably still meaningful given how large Starbucks is; a few percentage points change in the probability of completing an offer may mean tens of millions of dollars a year in additional customer spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percent of completed offers among customers who viewed offers: ' + str(round(offer_data.groupby('saw_offer').completed.mean()[1]*100, 1)) + '%')\n",
    "print('Percent of completed offers among customers who did not view offers: ' + str(round(offer_data.groupby('saw_offer').completed.mean()[0]*100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other possibility, however, is that learning about an offer did not have much impact on average, but it had a larger impact on certain customer segments or for certain offers. In other words, it would be worth knowing whether the probability of completing the offer depends on viewing it more for some customer segments and less for others (same for specific offers). \n",
    "\n",
    "There are different ways to set up this analysis. To make it more straightforward, I apply the same model as above separately to (a) those who saw offers before they had a chance to complete them, and (b) those who did not see offers at all or saw them only after completing them. The code below runs the model twice for these two subsamples and plots the coefficients from two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y\n",
    "offer_data_s_X = offer_data[\n",
    "    offer_data.saw_offer == 1][['user_id'] + x_vars]\n",
    "offer_data_ds_X = offer_data[\n",
    "    offer_data.saw_offer == 0][['user_id'] + x_vars]\n",
    "offer_data_s_y = offer_data[\n",
    "    offer_data.saw_offer == 1]['completed']\n",
    "offer_data_ds_y = offer_data[\n",
    "    offer_data.saw_offer == 0]['completed']\n",
    "\n",
    "# train and test 1\n",
    "X_train, y_train, X_test, y_test = train_test_split_by_user(\n",
    "    offer_data_s_X, offer_data_s_y)\n",
    "\n",
    "# fit 1\n",
    "completion_coefs_s = fit_test(logit_cv.best_estimator_, return_coefs=True)\n",
    "\n",
    "# train and test 2\n",
    "X_train, y_train, X_test, y_test = train_test_split_by_user(\n",
    "    offer_data_ds_X, offer_data_ds_y)\n",
    "\n",
    "# fit 2\n",
    "completion_coefs_ds = fit_test(logit_cv.best_estimator_, return_coefs=True)\n",
    "\n",
    "# combine coefficients into one data frame\n",
    "completion_coefs_sds = pd.concat([completion_coefs_s, \n",
    "                                 completion_coefs_ds],\n",
    "                                 axis=1)\n",
    "completion_coefs_sds.columns = ['Saw', 'Did not see']\n",
    "\n",
    "# plot\n",
    "completion_coefs_sds.plot.barh(figsize=[15,7], \n",
    "                               ylabel='Logistic regression coefficient', \n",
    "                               title='What predicts offer completion?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we can see that in many cases, there was not much difference in terms of the probability of completing offers in specific customer segments depending on whether customers viewed the offer or not. For example, the coefficients for females or customers in different age groups were about the same in the group that viewed offers and in the group that did not view them. Here are a few cases that stood out and would be worth highlighting:\n",
    "- Those who have been members for more than 1 year were substantially more likely to complete offers when they saw promos than when they did not.\n",
    "- In all income groups above 40K, but especially so in groups between 60K and 100K, customers who saw offers were *less* likely to complete these offers than when they did not see the offers. Not quite clear why this pattern might emerge.\n",
    "- With respect to specific offers, all discounts were clearly more likely to be redeemed when they were seen, whereas with BOGOs, this was not always the case. This may suggest, again, that discounts were generally more attractive to customers than BOGO offers.\n",
    "\n",
    "Except for these cases, however, the differences in completion probabilities between those who viewed offers and those who did not were not dramatic. This may suggest two possibilities: (1) promo offers in general were not extremely motivating, possibly because many customers were spending enough money anyway; (2) viewing the offer does not correctly reflect actual awareness of the offer, as customers could have learned about these offers in other ways. We will return to this issue at the end. \n",
    "\n",
    "## Predicting The Probability of Viewing Offers\n",
    "\n",
    "If we want to maximize the effectiveness of promo offers, not only we need to know who is more likely to use them when aware of them, but we also need to know who is more likely to see these offers. For this reason, it is worth conducting an analogous analysis of the probability of *viewing* the offers. The next code chunk does this and plots the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y\n",
    "data_viewing_X = offer_data[['user_id'] + x_vars]\n",
    "data_viewing_y = offer_data['viewed']\n",
    "\n",
    "# train and test\n",
    "X_train, y_train, X_test, y_test = train_test_split_by_user(data_viewing_X,\n",
    "                                                            data_viewing_y)\n",
    "\n",
    "# fit \n",
    "viewing_coefs = fit_test(logit_cv.best_estimator_, return_coefs=True)\n",
    "\n",
    "# plot\n",
    "viewing_coefs.plot.barh(figsize=[15,5], \n",
    "                        ylabel='Logistic regression coefficient', \n",
    "                        title='What predicts the viewing of offers?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Several things to highlight on this plot:\n",
    "- Not much difference in the probability of viewing depending on membership duration.\n",
    "- Not much difference between males and females.\n",
    "- Customers with incomes between 40K and 100K were more likely to view offers (especially those in the 60K-100K range) than both the wealthiest (over 100K) and the poorest (less than 40K) customers. The wealthiest may not care for promo offers much, and the poorest may not always be able to afford completing them.\n",
    "- Customers over 30 were more likely to view the offers than younger customers.\n",
    "- Those who completed offers before were more likely to view new offers.\n",
    "- Some offers were much less likely to be viewed than others, but there is no clear pattern in terms of, say, discounts vs BOGO offers or the amount of the reward.\n",
    "\n",
    "This might suggest that some additional efforts might be needed to make the offers more visible to certain customer segments (younger or poorer customers), but it is important first to investigate why these customers pay less attention to promotional activities. It could be, for example, that they believe the offers are generally not worth their time, and then changing the structure of these offers might help.\n",
    "\n",
    "# Discussion\n",
    "\n",
    "\n",
    "\n",
    "Viewing probably a flawed measure of awareness."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63988c7776bd4b2216d881fd2ad90ced5e2987f9f8c410eebe313e5d7f3b1f68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
